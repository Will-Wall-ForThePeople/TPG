import pandas as pd
import logging, pytz, io, os
from azure.identity import ManagedIdentityCredential, AzureCliCredential, ClientSecretCredential
from azure.keyvault.secrets import SecretClient
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from datetime import datetime, timedelta
from io import StringIO
from dotenv import load_dotenv

load_dotenv()

# Set up logging to both a file and the console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler("debug.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Convert UTC timestamp string to US Eastern Time
def convert_to_est(utc_time_str):
    """Convert UTC timestamp to EST/EDT"""
    try:
        # Handle different datetime formats
        if 'T' in utc_time_str and utc_time_str.endswith('Z'):
            # Format: "2024-01-15T12:30:45.000Z"
            utc_time = datetime.strptime(utc_time_str, "%Y-%m-%dT%H:%M:%S.%fZ")
        elif 'T' in utc_time_str:
            # Format: "2024-01-15T12:30:45"
            utc_time = datetime.strptime(utc_time_str.split('.')[0], "%Y-%m-%dT%H:%M:%S")
        else:
            # Try to parse as-is
            utc_time = pd.to_datetime(utc_time_str)
        
        utc_time = utc_time.replace(tzinfo=pytz.UTC)
        est = pytz.timezone("US/Eastern")
        est_time = utc_time.astimezone(est)
        return est_time
    except Exception as e:
        logger.warning(f"Could not convert timestamp {utc_time_str}: {e}")
        return pd.to_datetime(utc_time_str)

def save_csv(container_client, df, path):
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False, float_format='%.2f')
    csv_data = csv_buffer.getvalue().encode('utf-8-sig')
    blob_client = container_client.get_blob_client(path)
    blob_client.upload_blob(csv_data, overwrite=True)
    logger.info(f"Saved CSV to {path} with {len(df)} rows")

def get_credential(enable_probe: bool = True):
    logger = logging.getLogger("auth")
    scope = "https://vault.azure.net/.default"
    try:
        mi = ManagedIdentityCredential()
        # probe a simple token request to confirm MI works (optional)
        if enable_probe:
            mi.get_token(scope)  # probe token
            logger.info("ManagedIdentityCredential selected (token probe OK).")
        else:
            logger.info("ManagedIdentityCredential selected (probe disabled).")
        return mi
    except Exception:
        pass

    try:
        cli = AzureCliCredential()
        if enable_probe:
            cli.get_token(scope)
            logger.info("AzureCliCredential selected (token probe OK).")
        else:
            logger.info("AzureCliCredential selected (probe disabled).")
        return cli
    except Exception:
        pass

def get_blob_service_client():
    """
    Primary path: fetch connection string from Key Vault.
    Fallback: use local dev connection string from env.
    """
    # Local dev override: direct connection string
    conn = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
    if conn:
        return BlobServiceClient.from_connection_string(conn)

    # Otherwise, use Key Vault to fetch the connection string
    credential = get_credential()
    azure_vault_url = 'https://mm-ccc-project-team-kv.vault.azure.net/'

    secret_client = SecretClient(vault_url=azure_vault_url, credential=credential)
    connection_string = secret_client.get_secret("azure-connection-string").value
    return BlobServiceClient.from_connection_string(connection_string)

# Azure setup
blob_service_client = get_blob_service_client()
container_client = blob_service_client.get_container_client("tpg-data")

# Download the blob data from warehouse
logger.info("Downloading TPG data from warehouse...")
tpg_blob = container_client.get_blob_client("MM_173_TPG_DATA")
tpg_data = tpg_blob.download_blob().readall()
tpg_df = pd.read_parquet(io.BytesIO(tpg_data), engine="pyarrow")

logger.info(f"Loaded {len(tpg_df)} rows from warehouse")

# Rename columns
tpg_df = tpg_df.rename(columns={
    "EVALUATION_DATE": "Evaluation Date",
    "AGENT": "Agent",
    "AGENTEMAIL": "AgentEmail",
    "SUPERVISOR": "Supervisor",
    "BEHAVIOR_EXT": "Behavior Text",
    "RESPONSE": "Response",
    "ISOPPORTUNITY": "IsOpportunity",
    "ISDEFECT": "IsDefect"
})

# Select columns to keep
columns_to_keep = ["Evaluation Date", "Agent", "AgentEmail", "Supervisor", 
                   "Behavior Text", "Response", "IsOpportunity", "IsDefect"]
tpg_df = tpg_df[columns_to_keep]

# Clean up the Evaluation Date column
logger.info("Cleaning up Evaluation Date column...")

# Convert string timestamps to datetime if needed
if tpg_df["Evaluation Date"].dtype == 'object':
    tpg_df["Evaluation Date"] = tpg_df["Evaluation Date"].apply(
        lambda x: convert_to_est(x) if pd.notna(x) and isinstance(x, str) else x
    )

# Convert to datetime
tpg_df["Evaluation Date"] = pd.to_datetime(tpg_df["Evaluation Date"], errors="coerce")

# Extract just the date part for grouping
tpg_df["Date"] = tpg_df["Evaluation Date"].dt.date

# Remove rows with invalid dates
invalid_dates = tpg_df[tpg_df["Date"].isna()]
if not invalid_dates.empty:
    logger.warning(f"Found {len(invalid_dates)} rows with invalid dates")

tpg_df = tpg_df.dropna(subset=["Date"])

logger.info(f"After date cleanup: {len(tpg_df)} rows")
logger.info(f"Date range: {tpg_df['Date'].min()} to {tpg_df['Date'].max()}")
logger.info(f"Unique dates in data: {tpg_df['Date'].nunique()}")

# Define target behaviors
target_behaviors = [
    "Builds rapport by engaging customer",
    "Confirms customer's satisfaction with call outcome",
    "Maintains call control to guide conversation",
    "Personalizes call by using customer's name",
    "Takes ownership & displays willingness to help"
]

# Filter for target behaviors
filtered_df = tpg_df[tpg_df['Behavior Text'].isin(target_behaviors)]
logger.info(f"Filtered to {len(filtered_df)} rows with target behaviors")

# IMPORTANT: Calculate behavior statistics PER DATE AND AGENT
logger.info("Calculating behavior statistics by date and agent...")
behavior_stats = filtered_df.groupby(['Date', 'Agent', 'Behavior Text'], as_index=False).agg({
    'IsDefect': 'sum',
    'IsOpportunity': 'sum'
})

behavior_stats['IsDefect'] = pd.to_numeric(behavior_stats['IsDefect'], errors='coerce').fillna(0)
behavior_stats['IsOpportunity'] = pd.to_numeric(behavior_stats['IsOpportunity'], errors='coerce').fillna(0)

# Calculate defect rate
behavior_stats['DefectRate'] = behavior_stats.apply(
    lambda row: round(row['IsDefect'] / row['IsOpportunity'], 2) 
    if row['IsOpportunity'] > 0 else 0, axis=1
)

# Create pivot table for behaviors - BY DATE AND AGENT
logger.info("Creating behavior pivot table...")
behavior_pivot = behavior_stats.pivot_table(
    index=['Date', 'Agent'],
    columns='Behavior Text',
    values='DefectRate',
    aggfunc='first'
).round(2)
behavior_pivot = behavior_pivot.reset_index()

# Calculate totals PER DATE AND AGENT
logger.info("Calculating totals by date and agent...")
totals = tpg_df.groupby(['Date', 'Agent'], as_index=False).agg({
    'IsDefect': 'sum',
    'IsOpportunity': 'sum'
})

totals['IsDefect'] = pd.to_numeric(totals['IsDefect'], errors='coerce').fillna(0)
totals['IsOpportunity'] = pd.to_numeric(totals['IsOpportunity'], errors='coerce').fillna(0)

# Calculate overall defect rate
totals['DefectRate'] = totals.apply(
    lambda row: round(row['IsDefect'] / row['IsOpportunity'], 2) 
    if row['IsOpportunity'] > 0 else 0, axis=1
)
totals = totals[['Date', 'Agent', 'DefectRate']]

# Calculate delight scores PER DATE AND AGENT
logger.info("Calculating delight scores by date and agent...")
delight_df = tpg_df[tpg_df['Behavior Text'] == "Customer demeanor at end of call"]
delight_df["Response"] = delight_df["Response"].astype(str).str.strip()

if not delight_df.empty:
    delight_counts = delight_df.groupby(['Date', 'Agent', 'Response'], dropna=False).size().unstack(
        'Response', fill_value=0
    ).reset_index()
    
    # Ensure all expected columns exist
    for col in ['Audibly happy', 'Neutral', 'Irate']:
        if col not in delight_counts.columns:
            delight_counts[col] = 0
    
    # Calculate delight percentage
    total_responses = delight_counts[['Audibly happy', 'Neutral', 'Irate']].sum(axis=1)
    delight_counts['Delight'] = delight_counts.apply(
        lambda row: round(row['Audibly happy'] / total_responses[row.name], 2) 
        if total_responses[row.name] > 0 else 0, axis=1
    )
    delight_counts = delight_counts[['Date', 'Agent', 'Delight']]
else:
    # Create empty delight dataframe if no data
    delight_counts = pd.DataFrame(columns=['Date', 'Agent', 'Delight'])

# Get agent information WITH DATE
logger.info("Compiling agent information by date...")
# Keep one row per agent per date with their info
agent_info = tpg_df.groupby(['Date', 'Agent']).agg({
    'Evaluation Date': 'max',  # Get the latest evaluation datetime for that date
    'AgentEmail': 'first',
    'Supervisor': 'first'
}).reset_index()

# Format the Evaluation Date for output
agent_info['Evaluation Date'] = agent_info['Evaluation Date'].dt.strftime('%Y-%m-%d %H:%M:%S')

# MERGE ALL DATAFRAMES - maintaining date granularity
logger.info("Merging all data...")
final_df = agent_info.merge(
    behavior_pivot, 
    on=['Date', 'Agent'], 
    how='left'
)
final_df = final_df.merge(
    totals, 
    on=['Date', 'Agent'], 
    how='left'
)
final_df = final_df.merge(
    delight_counts, 
    on=['Date', 'Agent'], 
    how='left'
)

# Fill NaN values with 0 for numeric columns
numeric_columns = final_df.select_dtypes(include=['float64', 'int64']).columns
final_df[numeric_columns] = final_df[numeric_columns].fillna(0)

# Sort by Date and Agent for better readability
final_df = final_df.sort_values(['Date', 'Agent'])

# Convert Date to string for CSV output
final_df['Date'] = final_df['Date'].astype(str)

logger.info(f"Final dataset has {len(final_df)} rows (agent-date combinations)")
logger.info(f"Unique dates: {final_df['Date'].nunique()}")
logger.info(f"Unique agents: {final_df['Agent'].nunique()}")
logger.info("Columns in final dataset:")
logger.info(final_df.columns.tolist())

# Save to blob storage - now includes multiple dates
storage_container = blob_service_client.get_container_client('centrical-data')
storage_path = f"TPG/CENTRICAL_TPG_WAREHOUSE.csv"

save_csv(storage_container, final_df, storage_path)
logger.info("Process completed successfully!")

# Display summary statistics
print("\n=== Summary Statistics ===")
print(f"Total rows: {len(final_df)}")
print(f"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}")
print(f"Unique dates: {final_df['Date'].nunique()}")
print(f"Unique agents: {final_df['Agent'].nunique()}")
print(f"\nSample of final data (first 10 rows):")
print(final_df.head(10))

# Show date distribution
print(f"\nRows per date:")
date_distribution = final_df.groupby('Date').size().sort_index()
print(date_distribution)

# Optional: Save separate files per date if needed
save_by_date = os.getenv("SAVE_BY_DATE", "false").lower() == "true"
if save_by_date:
    logger.info("Saving separate files by date...")
    for date in final_df['Date'].unique():
        date_df = final_df[final_df['Date'] == date]
        date_str = date.replace('-', '')
        date_path = f"TPG/Daily/CENTRICAL_TPG_{date_str}.csv"
        save_csv(storage_container, date_df, date_path)
        logger.info(f"Saved {len(date_df)} rows for date {date}")
