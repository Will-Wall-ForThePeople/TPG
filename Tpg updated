import pandas as pd
import logging, pytz, io, os
from azure.identity import ManagedIdentityCredential, AzureCliCredential, ClientSecretCredential
from azure.keyvault.secrets import SecretClient
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from datetime import datetime, timedelta
from io import StringIO
from dotenv import load_dotenv

load_dotenv()

# Set up logging to both a file and the console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler("debug.log"),
        logging.StreamHandler()
    ]
)

# Convert UTC timestamp string to US Eastern Time
def convert_to_est(utc_time_str):
    """Convert UTC timestamp to EST/EDT"""
    try:
        # Handle different datetime formats
        if 'T' in utc_time_str and utc_time_str.endswith('Z'):
            # Format: "2024-01-15T12:30:45.000Z"
            utc_time = datetime.strptime(utc_time_str, "%Y-%m-%dT%H:%M:%S.%fZ")
        elif 'T' in utc_time_str:
            # Format: "2024-01-15T12:30:45"
            utc_time = datetime.strptime(utc_time_str.split('.')[0], "%Y-%m-%dT%H:%M:%S")
        else:
            # Try to parse as-is
            utc_time = pd.to_datetime(utc_time_str)
        
        utc_time = utc_time.replace(tzinfo=pytz.UTC)
        est = pytz.timezone("US/Eastern")
        est_time = utc_time.astimezone(est)
        return est_time.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        logging.warning(f"Could not convert timestamp {utc_time_str}: {e}")
        return utc_time_str

def save_csv(container_client, df, path):
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False, float_format='%.2f')
    csv_data = csv_buffer.getvalue().encode('utf-8-sig')
    blob_client = container_client.get_blob_client(path)
    blob_client.upload_blob(csv_data, overwrite=True)
    logging.info(f"Saved CSV to {path}")

def get_credential(enable_probe: bool = True):
    logger = logging.getLogger("auth")
    scope = "https://vault.azure.net/.default"
    try:
        mi = ManagedIdentityCredential()
        # probe a simple token request to confirm MI works (optional)
        if enable_probe:
            mi.get_token(scope)  # probe token
            logger.info("ManagedIdentityCredential selected (token probe OK).")
        else:
            logger.info("ManagedIdentityCredential selected (probe disabled).")
        return mi
    except Exception:
        pass

    try:
        cli = AzureCliCredential()
        if enable_probe:
            cli.get_token(scope)
            logger.info("AzureCliCredential selected (token probe OK).")
        else:
            logger.info("AzureCliCredential selected (probe disabled).")
        return cli
    except Exception:
        pass

def get_blob_service_client():
    """
    Primary path: fetch connection string from Key Vault.
    Fallback: use local dev connection string from env.
    """
    # Local dev override: direct connection string
    conn = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
    if conn:
        return BlobServiceClient.from_connection_string(conn)

    # Otherwise, use Key Vault to fetch the connection string
    credential = get_credential()
    azure_vault_url = 'https://mm-ccc-project-team-kv.vault.azure.net/'

    secret_client = SecretClient(vault_url=azure_vault_url, credential=credential)
    # Secret name expected: "azure-connection-string"
    connection_string = secret_client.get_secret("azure-connection-string").value
    return BlobServiceClient.from_connection_string(connection_string)

# Azure setup
blob_service_client = get_blob_service_client()
container_client = blob_service_client.get_container_client("tpg-data")

# Download the blob data
logging.info("Downloading TPG data...")
tpg_blob = container_client.get_blob_client("MM_173_TPG_DATA")
tpg_data = tpg_blob.download_blob().readall()
tpg_df = pd.read_parquet(io.BytesIO(tpg_data), engine="pyarrow")

logging.info(f"Loaded {len(tpg_df)} rows")

# Rename columns
tpg_df = tpg_df.rename(columns={
    "EVALUATION_DATE": "Evaluation Date",
    "AGENT": "Agent",
    "AGENTEMAIL": "AgentEmail",
    "SUPERVISOR": "Supervisor",
    "BEHAVIOR_EXT": "Behavior Text",
    "RESPONSE": "Response",
    "ISOPPORTUNITY": "IsOpportunity",
    "ISDEFECT": "IsDefect"
})

# Select columns to keep
columns_to_keep = ["Evaluation Date", "Agent", "AgentEmail", "Supervisor", 
                   "Behavior Text", "Response", "IsOpportunity", "IsDefect"]
tpg_df = tpg_df[columns_to_keep]

# Clean up the Evaluation Date column
logging.info("Cleaning up Evaluation Date column...")

# First, check if the date column contains string timestamps
if tpg_df["Evaluation Date"].dtype == 'object':
    # If dates are strings with timezone info, convert them
    tpg_df["Evaluation Date"] = tpg_df["Evaluation Date"].apply(
        lambda x: convert_to_est(x) if pd.notna(x) and isinstance(x, str) else x
    )

# Convert to datetime
tpg_df["Evaluation Date"] = pd.to_datetime(tpg_df["Evaluation Date"], errors="coerce")

# Extract just the date part for grouping
tpg_df["Date"] = tpg_df["Evaluation Date"].dt.date

# Log any rows with invalid dates
invalid_dates = tpg_df[tpg_df["Date"].isna()]
if not invalid_dates.empty:
    logging.warning(f"Found {len(invalid_dates)} rows with invalid dates")
    logging.info("Sample of invalid date rows:")
    logging.info(invalid_dates.head())

# Remove rows with invalid dates
tpg_df = tpg_df.dropna(subset=["Date"])

logging.info(f"After date cleanup: {len(tpg_df)} rows")
logging.info(f"Date range: {tpg_df['Date'].min()} to {tpg_df['Date'].max()}")

# Define target behaviors
target_behaviors = [
    "Builds rapport by engaging customer",
    "Confirms customer's satisfaction with call outcome",
    "Maintains call control to guide conversation",
    "Personalizes call by using customer's name",
    "Takes ownership & displays willingness to help"
]

# Filter for target behaviors
filtered_df = tpg_df[tpg_df['Behavior Text'].isin(target_behaviors)]
logging.info(f"Filtered to {len(filtered_df)} rows with target behaviors")

# Calculate behavior statistics
behavior_stats = filtered_df.groupby(['Date', 'Agent', 'Behavior Text'], as_index=False).agg({
    'IsDefect': 'sum',
    'IsOpportunity': 'sum'
})

behavior_stats['IsDefect'] = pd.to_numeric(behavior_stats['IsDefect'], errors='coerce')
behavior_stats['IsOpportunity'] = pd.to_numeric(behavior_stats['IsOpportunity'], errors='coerce')

# Calculate defect rate, handling division by zero
behavior_stats['DefectRate'] = behavior_stats.apply(
    lambda row: round(row['IsDefect'] / row['IsOpportunity'], 2) 
    if row['IsOpportunity'] > 0 else 0, axis=1
)

# Calculate totals per agent
totals = tpg_df.groupby(['Agent'], as_index=False).agg({
    'IsDefect': 'sum',
    'IsOpportunity': 'sum'
})

totals['IsDefect'] = pd.to_numeric(totals['IsDefect'], errors='coerce')
totals['IsOpportunity'] = pd.to_numeric(totals['IsOpportunity'], errors='coerce')

# Calculate overall defect rate per agent
totals['DefectRate'] = totals.apply(
    lambda row: round(row['IsDefect'] / row['IsOpportunity'], 2) 
    if row['IsOpportunity'] > 0 else 0, axis=1
)
totals = totals[['Agent', 'DefectRate']]

# Create pivot table for behaviors
behavior_pivot = behavior_stats.pivot_table(
    index='Agent',
    columns='Behavior Text',
    values='DefectRate',
    aggfunc='first'
).round(2)
behavior_pivot = behavior_pivot.reset_index()

# Calculate delight scores
delight_df = tpg_df[tpg_df['Behavior Text'] == "Customer demeanor at end of call"]
delight_df["Response"] = delight_df["Response"].astype(str).str.strip()

delight_counts = delight_df.groupby(["Agent", "Response"], dropna=False).size().unstack(
    "Response", fill_value=0
).reset_index()

# Calculate delight percentage
if 'Audibly happy' in delight_counts.columns:
    total_responses = delight_counts[['Audibly happy', 'Neutral', 'Irate']].sum(axis=1)
    delight_counts['Delight'] = delight_counts.apply(
        lambda row: round(row['Audibly happy'] / total_responses[row.name], 2) 
        if total_responses[row.name] > 0 else 0, axis=1
    )
else:
    delight_counts['Delight'] = 0

delight_counts = delight_counts[['Agent', 'Delight']]

# Get unique agent information with the most recent evaluation date
df_unique = tpg_df.sort_values('Evaluation Date').groupby('Agent').last().reset_index()
df_unique = df_unique[["Evaluation Date", "Agent", "AgentEmail", "Supervisor"]]

# Format the Evaluation Date for output
df_unique['Evaluation Date'] = df_unique['Evaluation Date'].dt.strftime('%Y-%m-%d %H:%M:%S')

# Merge all dataframes
final_df = df_unique.merge(behavior_pivot, on='Agent', how='left')
final_df = final_df.merge(totals, on='Agent', how='left')
final_df = final_df.merge(delight_counts, on='Agent', how='left')

# Fill NaN values with 0 for numeric columns
numeric_columns = final_df.select_dtypes(include=['float64', 'int64']).columns
final_df[numeric_columns] = final_df[numeric_columns].fillna(0)

logging.info(f"Final dataset has {len(final_df)} agents")
logging.info("Columns in final dataset:")
logging.info(final_df.columns.tolist())

# Save to blob storage
storage_container = blob_service_client.get_container_client('centrical-data')
storage_path = f"TPG/CENTRICAL_TPG.csv"

save_csv(storage_container, final_df, storage_path)
logging.info("Process completed successfully!")

# Display summary statistics
print("\n=== Summary Statistics ===")
print(f"Total agents: {len(final_df)}")
print(f"Date range: {tpg_df['Date'].min()} to {tpg_df['Date'].max()}")
print(f"\nSample of final data:")
print(final_df.head())
